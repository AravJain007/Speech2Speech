{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tinY3Rd53XXL"
      },
      "outputs": [],
      "source": [
        "!pip install -q streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-7POjEp33ox",
        "outputId": "b1f98dda-d0bd-4662-c800-40e847f96023"
      },
      "outputs": [],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylgbsKi_5LF5",
        "outputId": "281643b3-cf85-4bcf-d68c-cddd04452319"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch torchaudio accelerate bitsandbytes streamlit gtts onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_W0Sumv8vOr",
        "outputId": "ccbdf9f0-f231-4193-82e1-f9b46eba2571"
      },
      "outputs": [],
      "source": [
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkCFLORf4GqN",
        "outputId": "2cf3050d-05a5-4170-e530-cec594297ad8"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import torch\n",
        "import torchaudio\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration, AutoTokenizer, AutoModelForCausalLM\n",
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "import streamlit as st\n",
        "import os\n",
        "import tempfile\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# VAD Optimization\n",
        "@st.cache_resource\n",
        "def load_vad_model():\n",
        "    model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
        "                                  model='silero_vad',\n",
        "                                  force_reload=True,\n",
        "                                  onnx=True)\n",
        "    return model, utils\n",
        "\n",
        "# Whisper Optimization\n",
        "@st.cache_resource\n",
        "def load_whisper_model():\n",
        "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
        "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\",\n",
        "                                                            device_map=\"auto\",\n",
        "                                                            load_in_8bit=True)\n",
        "    return processor, model\n",
        "\n",
        "# LLM Optimization (Phi-2 as example)\n",
        "@st.cache_resource\n",
        "def load_llm_model():\n",
        "    model_name = \"microsoft/phi-2\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                                 device_map=\"auto\",\n",
        "                                                 trust_remote_code=True,\n",
        "                                                 load_in_4bit=True,\n",
        "                                                 max_memory={0: \"15GB\"})\n",
        "    return tokenizer, model\n",
        "\n",
        "# TTS Optimization (using gTTS instead of Parler TTS for simplicity and speed)\n",
        "from gtts import gTTS\n",
        "import io\n",
        "\n",
        "def synthesize_speech(text, output_file=\"output.mp3\"):\n",
        "    tts = gTTS(text=text, lang='en', slow=False)\n",
        "    mp3_fp = io.BytesIO()\n",
        "    tts.write_to_fp(mp3_fp)\n",
        "    mp3_fp.seek(0)\n",
        "    with open(output_file, 'wb') as f:\n",
        "        f.write(mp3_fp.getvalue())\n",
        "    return output_file\n",
        "\n",
        "# Load models\n",
        "vad_model, vad_utils = load_vad_model()\n",
        "whisper_processor, whisper_model = load_whisper_model()\n",
        "llm_tokenizer, llm_model = load_llm_model()\n",
        "\n",
        "def vad_and_split(audio_file, threshold=0.5):\n",
        "    waveform, sample_rate = torchaudio.load(audio_file)\n",
        "    if waveform.shape[0] > 1:  # Convert stereo to mono\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "    get_speech_timestamps = vad_utils[0]\n",
        "    speech_timestamps = get_speech_timestamps(waveform[0], vad_model, threshold=threshold)\n",
        "    return waveform, sample_rate, speech_timestamps\n",
        "\n",
        "def transcribe(waveform, sample_rate):\n",
        "    input_features = whisper_processor(waveform.numpy()[0], sampling_rate=sample_rate, return_tensors=\"pt\").input_features\n",
        "    input_features = input_features.to(whisper_model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predicted_ids = whisper_model.generate(input_features)\n",
        "    transcription = whisper_processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "    return transcription[0]\n",
        "\n",
        "def generate_response(text):\n",
        "    prefix = \"\"\"System: You are an expert at one-line answers. You never answer or explain any more than asked by the user.\n",
        "    Your answers are always crisp, to the point, and extremely accurate.\n",
        "    If the user query is very long to answer, then summarize it and then answer the user.\n",
        "    User Query: \"\"\"\n",
        "    postfix = \"\\nAssistant: \"\n",
        "\n",
        "    full_prompt = prefix + text + postfix\n",
        "\n",
        "    inputs = llm_tokenizer(full_prompt, return_tensors=\"pt\", max_length=4096, truncation=True).to(llm_model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = llm_model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_new_tokens=128,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "    response = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response_cleaned = response[len(full_prompt):].strip()\n",
        "\n",
        "    return response_cleaned\n",
        "\n",
        "def process_audio_pipeline(audio_file):\n",
        "    waveform, sample_rate, speech_timestamps = vad_and_split(audio_file)\n",
        "\n",
        "    # Extract speech segments\n",
        "    speech_segments = [waveform[:, start:end] for start, end in speech_timestamps]\n",
        "\n",
        "    if not speech_segments:\n",
        "        # No speech detected, use the entire audio\n",
        "        full_speech = waveform\n",
        "    else:\n",
        "        full_speech = torch.cat(speech_segments, dim=1)\n",
        "\n",
        "    transcription = transcribe(full_speech, sample_rate)\n",
        "    response = generate_response(transcription.strip())\n",
        "    output_file = synthesize_speech(response)\n",
        "\n",
        "    return output_file, transcription, response\n",
        "\n",
        "def is_valid_audio(file):\n",
        "    try:\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:\n",
        "            tmp_file.write(file.getvalue())\n",
        "            tmp_file_path = tmp_file.name\n",
        "\n",
        "        audio = AudioSegment.from_wav(tmp_file_path)\n",
        "        os.unlink(tmp_file_path)  # Delete the temporary file\n",
        "\n",
        "        # Check if the audio duration is between 1 second and 5 minutes\n",
        "        duration_ms = len(audio)\n",
        "        if 1000 <= duration_ms <= 300000:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "# Streamlit app\n",
        "st.title(\"Optimized Speech-to-Speech Pipeline\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Choose an audio file\", type=['wav', 'mp3'])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    if is_valid_audio(uploaded_file):\n",
        "        st.audio(uploaded_file, format='audio/wav')\n",
        "\n",
        "        if st.button(\"Process Audio\"):\n",
        "            with st.spinner(\"Processing...\"):\n",
        "                # Save the uploaded file temporarily\n",
        "                with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:\n",
        "                    tmp_file.write(uploaded_file.getvalue())\n",
        "                    temp_path = tmp_file.name\n",
        "\n",
        "                try:\n",
        "                    output_file, transcription, response = process_audio_pipeline(temp_path)\n",
        "\n",
        "                    st.success(\"Processing completed!\")\n",
        "\n",
        "                    st.subheader(\"Transcription\")\n",
        "                    st.write(transcription)\n",
        "\n",
        "                    st.subheader(\"Generated Response\")\n",
        "                    st.write(response)\n",
        "\n",
        "                    st.subheader(\"Generated Audio\")\n",
        "                    st.audio(output_file, format='audio/mp3')\n",
        "                except Exception as e:\n",
        "                    st.error(f\"An error occurred during processing: {str(e)}\")\n",
        "                finally:\n",
        "                    # Clean up the temporary file\n",
        "                    os.unlink(temp_path)\n",
        "    else:\n",
        "        st.error(\"The uploaded file is not a valid audio file or its duration is not between 1 second and 5 minutes. Please upload a valid WAV or MP3 file.\")\n",
        "else:\n",
        "    st.warning(\"Please upload an audio file.\")\n",
        "\n",
        "st.sidebar.title(\"About\")\n",
        "st.sidebar.info(\"This app demonstrates an optimized speech-to-speech pipeline using VAD, Whisper, Phi-2, and gTTS models.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Lpjt6BU7ZSN",
        "outputId": "5024f8f2-ef6c-477b-a444-3b86bbe8d49f"
      },
      "outputs": [],
      "source": [
        "!ngrok config add-authtoken 2lF8Y1iKK9n3lKlQuvim1KVsDpn_5dahJHKY8qasyJCq3RyBf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "-dlX4bSw5uZD",
        "outputId": "a6368596-fb08-4c27-c886-823c08d1414c"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Start Streamlit in the background\n",
        "streamlit_process = subprocess.Popen([\"streamlit\", \"run\", \"app.py\"])\n",
        "\n",
        "# Wait for Streamlit to start up\n",
        "time.sleep(10)\n",
        "\n",
        "# Set up ngrok\n",
        "ngrok.set_auth_token(\"YOUR_NGROK_AUTH_TOKEN\")  # Replace with your ngrok auth token\n",
        "public_url = ngrok.connect(8501).public_url\n",
        "\n",
        "print(f\"Streamlit app is running on: {public_url}\")\n",
        "\n",
        "# Keep the notebook running\n",
        "!sleep 1h"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
